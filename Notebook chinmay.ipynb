{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__doc__=\"\"\" PROJECT : HUMANIOD ROBOTICS \n",
    "\n",
    "Project-A\n",
    "\n",
    "1.Take a Photo of any random scene containing some objects. \n",
    "Use Template Matching and feature based template matching to detect objects.  \n",
    "\n",
    "2.Take a photo of any random scene containing objects. Detects bounding boxes for the objects in the scene using any object detection technique.\n",
    "Use the midpoint of the bounding box to measure the angular orientation in vertical and horizontal direction of each object from the centerline of the camera.  \n",
    "Hint: Use Field of View values for reference.\n",
    "\n",
    "3.Make a video of 15 seconds. Split it into different frames by taking frequencies of 10, 15, 20, 25 frames per second. \n",
    "\n",
    "\n",
    "4.Make a video of 15 seconds with a moving camera while tracking an object. \n",
    "Detect that object in each frame and draw the location of the center of the bounding box.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.Take a Photo of any random scene containing some objects. \\nUse Template Matching and feature based template matching to detect objects.  '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"1.Take a Photo of any random scene containing some objects. \n",
    "Use Template Matching and feature based template matching to detect objects.  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.5.4.58)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.20.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.5.4.58)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-contrib-python) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Implementation of Template Matching\n",
    "import numpy #this module helps to deal with arrays and matrices easily.\n",
    "\n",
    "import cv2 #importing opencv library\n",
    "\n",
    "#reading the testing image through open cv\n",
    "colored_image = cv2.imread('test_image_f.jpg')\n",
    "#converting the image to BGR to grey \n",
    "grey_image = cv2.cvtColor(colored_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#reading testing template image\n",
    "test_template = cv2.imread('test_template_f.jpg', 0)\n",
    "\n",
    "width, height = test_template.shape[::-1]   \n",
    "\n",
    "resolution = cv2.matchTemplate(grey_image, test_template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "# setting\n",
    "\n",
    "threshold = 0.6\n",
    "\n",
    "localization = numpy.where(resolution >= threshold)\n",
    "\n",
    "for alpha in zip(*localization[::-1]):\n",
    "\n",
    "    cv2.rectangle(colored_image, alpha, (alpha[0] + width, alpha[1] + height), (0, 253, 0), 1)\n",
    "\n",
    "cv2.imshow('TEMPLATE DETECTED', colored_image),cv2.waitKey(0); cv2.destroyAllWindows(); cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#implementation of Feature Matching\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "image_1=cv2.imread('letusc_camera1.jpg')\n",
    "image_2 = cv2.imread('letusc_camera.jpg')\n",
    "\n",
    "#ORB has by default uses 500 features\n",
    "\n",
    "kep_point_1, descriptor_1 = cv2.ORB_create(nfeatures=100000).detectAndCompute(image_1,None);\n",
    "keypoint_2, descriptor_2 = cv2.ORB_create(nfeatures=100000).detectAndCompute(image_2,None);\n",
    "\n",
    "\n",
    "\n",
    "brute_force_matches = cv2.BFMatcher().knnMatch(descriptor_1,descriptor_2,k=2);\n",
    "\n",
    "good_features = []\n",
    "for alpha,beta in brute_force_matches:\n",
    "    if alpha.distance< 0.75*beta.distance:\n",
    "        good_features.append([alpha]);\n",
    "print(len(good_features))\n",
    "image_3 = cv2.drawMatchesKnn(image_1,kep_point_1,image_2,keypoint_2,good_features,None,flags=2)\n",
    "\n",
    "image_keypoint_1 = cv2.drawKeypoints(image_1,kep_point_1,None);\n",
    "image_keypoint_2 = cv2.drawKeypoints(image_2,keypoint_2,None);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow(\"image_3\",image_3)\n",
    "cv2.waitKey(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100 mistakes', '100 TIPS TO CRACK IIT', 'ANSI C INTRODUCTION TO PROGRAMMING', 'GENERAL KNOWLEDGE 2015', 'IM THE MIND', 'INTRODUTION TO ALGORITHMS', 'NCC', 'random Curiosity', 'ShowPdf']\n",
      "Existing Files 9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "location = 'Q1'\n",
    "\n",
    "\n",
    "array_images = [] #\"\"\"defined to generate a array of immages\"\"\"\n",
    "\n",
    "file_name= [] #\"\"\"defined to merge file name in an array\"\"\"\n",
    "\n",
    "Playlist = os.listdir(location)# \"\"\"redirecting to the location of  playlist in Q2 directory\"\"\"\n",
    "\n",
    "#\"\"\"printing all existing files in the playlist\"\"\"\n",
    "\n",
    "for file_ in Playlist:\n",
    "\n",
    "    get_file = cv2.imread(f'{location}/{file_}',0)\n",
    "\n",
    "    array_images.append(get_file)\n",
    "\n",
    "    file_name.append(os.path.splitext(file_)[0])\n",
    "\n",
    "print(file_name)\n",
    "\n",
    "\n",
    "print('Existing Files',len(Playlist)) #number of existing files printed\n",
    "\n",
    "def detect_descriptor(array_images):\n",
    "\n",
    "    descriptor_list=[]\n",
    "\n",
    "    for image_ in array_images:\n",
    "\n",
    "        keypoint,descriptor = cv2.ORB_create().detectAndCompute(image_,None)\n",
    "\n",
    "        descriptor_list.append(descriptor)\n",
    "\n",
    "    return descriptor_list\n",
    "\n",
    "def detect_name_(image_, descriptor_list,value_threshold=12):\n",
    "\n",
    "    keypoint_2,descriptor_2 = cv2.ORB_create().detectAndCompute(image_,None)\n",
    "\n",
    "    point_list=[]\n",
    "\n",
    "    End_value = -1\n",
    "\n",
    "    try:\n",
    "\n",
    "        for descriptor in descriptor_list:\n",
    "            \n",
    "            best_match = []\n",
    "\n",
    "            for x, y in cv2.BFMatcher().knnMatch(descriptor, descriptor_2, k=2):\n",
    "\n",
    "                if x.distance < 0.75*y.distance:\n",
    "\n",
    "                    best_match.append([x])\n",
    "\n",
    "            point_list.append(len(best_match))\n",
    "    except:\n",
    "         pass\n",
    "   \n",
    "    if len(point_list)!=0:\n",
    "\n",
    "         if max(point_list) > value_threshold:\n",
    "\n",
    "             End_value = point_list.index(max(point_list))\n",
    "\n",
    "    return End_value     \n",
    "                                        \n",
    "\n",
    "descriptor_list = detect_descriptor(array_images)\n",
    "\n",
    "print(len(descriptor_list))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    final, file_mv = cap.read()\n",
    "\n",
    "    coloured_file = file_mv.copy()\n",
    "\n",
    "    file_mv = cv2.cvtColor(file_mv,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    name_ = detect_name_(file_mv,descriptor_list)\n",
    "\n",
    "    if name_ != -1:\n",
    "        \n",
    "        cv2.putText(coloured_file,file_name[name_],(50,50),cv2.FONT_HERSHEY_SIMPLEX , 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('file_mv',coloured_file)\n",
    "\n",
    "    cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2.Take a photo of any random scene containing objects. Detects bounding boxes for the objects in the scene using any object detection technique.\n",
    "Use the midpoint of the bounding box to measure the angular orientation in vertical and horizontal direction of each object from the centerline of the camera.  \n",
    "Hint: Use Field of View values for reference.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 \n",
    "\n",
    "import numpy\n",
    " \n",
    "#reading the image consist of All objects to be detected\n",
    "object_file = cv2.imread(\"objects.jpg\")\n",
    " \n",
    "#greyscale conversion of the file\n",
    "grey_image = cv2.cvtColor(object_file, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "#turning the image to binary\n",
    "_, binary_image = cv2.threshold(grey_image, 50, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    " \n",
    "#detecting all existing contours in the given image.\n",
    "detect_all_contours, _ = cv2.findContours(binary_image, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    " \n",
    "for iterator_, contour in enumerate(detect_all_contours): #enumerate is an default iterator that begins with zero\n",
    " \n",
    "  #summing up area of current existing contour\n",
    "  area = cv2.contourArea(contour)\n",
    " \n",
    "  #restricting area to 2550 to 100000\n",
    "  if area < 2500 or 100000 < area:\n",
    "    continue\n",
    " \n",
    "  \n",
    "  quad_shape = cv2.minAreaRect(contour)\n",
    "\n",
    "  covering_area = cv2.boxPoints(quad_shape)\n",
    "\n",
    "  covering_area = numpy.int0(covering_area)\n",
    " \n",
    "  \n",
    "  obj_centre = (int(quad_shape[0][0]),int(quad_shape[0][1])) \n",
    "\n",
    "  obj_width = int(quad_shape[1][0])\n",
    "\n",
    "  obj_length = int(quad_shape[1][1])\n",
    "  \n",
    "  inclination = int(quad_shape[2])\n",
    " \n",
    "     \n",
    "  if obj_width < obj_length:\n",
    "\n",
    "    inclination = 90 - inclination\n",
    "\n",
    "  else:\n",
    "    inclination = 180-inclination\n",
    "         \n",
    "  text_ = \"  Angular orientation:\" + str(inclination) + \"degrees\"\n",
    "\n",
    "  text_bacground = cv2.rectangle(object_file, (obj_centre[0]-35, obj_centre[1]-25), \n",
    "    (obj_centre[0] + 300, obj_centre[1] + 10), (0,0,0), -1)\n",
    "\n",
    "  cv2.putText(object_file, text_, (obj_centre[0]-50, obj_centre[1]), \n",
    "\n",
    "    cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 0.8, (255,255,255), 1,cv2.LINE_AA)\n",
    "\n",
    "  cv2.drawContours(object_file,[covering_area],0,(255,50,200),2)\n",
    " \n",
    "cv2.imshow('Output Image', object_file)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "  \n",
    "cv2.imwrite(\"Output Image_Q2.jpg\", object_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3\"\"\"Make a video of 15 seconds. Split it into different frames by taking frequencies of 10, 15, 20, 25 frames per second.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('car-overhead-1.avi')\n",
    "\n",
    "cv2.namedWindow('Display_frame',0)\n",
    "\n",
    "cv2.resizeWindow('Display_frame',300,300)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "out1 = cv2.VideoWriter('output1.avi', fourcc, 10.0, (300,300))\n",
    "\n",
    "out2 = cv2.VideoWriter('output2.avi', fourcc, 15.0, (300,300))\n",
    "\n",
    "out3 = cv2.VideoWriter('output3.avi', fourcc, 20.0, (300,300))\n",
    "\n",
    "out4 = cv2.VideoWriter('output4.avi', fourcc, 25.0, (300,300))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    val_return, Display_frame = cap.read()\n",
    "    \n",
    "    if val_return:\n",
    "        vidout=cv2.resize(Display_frame,(300,300))\n",
    "\n",
    "\n",
    "        out1.write(vidout)\n",
    "\n",
    "        out2.write(vidout)\n",
    "\n",
    "        out3.write(vidout)\n",
    "\n",
    "        out4.write(vidout)\n",
    "\n",
    "        cv2.imshow('Display_frame',Display_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "\n",
    "out1.release()\n",
    "\n",
    "out2.release()\n",
    "\n",
    "out3.release()\n",
    "\n",
    "out4.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.Make a video of 15 seconds with a moving camera while tracking an object. \\nDetect that object in each frame and draw the location of the center of the bounding box.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"4.Make a video of 15 seconds with a moving camera while tracking an object. \n",
    "Detect that object in each frame and draw the location of the center of the bounding box.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2 \n",
    "import numpy \n",
    " \n",
    "def main():\n",
    "#Captured the video file using the video capture , we can also use webcam for real time object tracking \n",
    "\n",
    "\n",
    " \n",
    "    # Capturing the video file using video capture\n",
    "    video_file = cv2.VideoCapture('car-overhead-1.avi');\n",
    "\n",
    "    #reading the estimated area size to be tracked\n",
    "    templte_image = cv2.imread('temp.jpg');\n",
    "    #converting the image to numpy array\n",
    "    image_numpy = numpy.array(templte_image);\n",
    "    # Background sub. Helps us to remove existing backgroung from the image which helps us to get the moving foreground easily from the background.\n",
    "\n",
    "    #to write the video as output file \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "    #generate createBackgroundSubtractorMOG2 from last 700 frames \n",
    "    back_sub = cv2.createBackgroundSubtractorMOG2(history=750,varThreshold=20, detectShadows=True)\n",
    " \n",
    "    #creating kerel and gen_kernel can be used to squeeze the dimensions \n",
    "    gen_kernel = numpy.ones((20,20),numpy.uint8)\n",
    "    # Next we used numpy.ones() to squeez or modify the existing dimensions and return a new array \n",
    "\n",
    "    output_car = cv2.VideoWriter('output_car.avi', fourcc, 20.0, (300,300))\n",
    "    while(True):\n",
    " \n",
    "        #capturing the video's each display_frame\n",
    "        return_vid, display_frame = video_file.read()\n",
    "        \n",
    "        # Tracked each frame of video \n",
    "\n",
    " \n",
    "        #deduce foreground mask using each display_frame\n",
    "        foreground_mask = back_sub.apply(display_frame)\n",
    "        # Deduced the foreground from the background \n",
    "\n",
    "\n",
    "        foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_CLOSE, gen_kernel)\n",
    " \n",
    "       #removing noise \n",
    "        foreground_mask = cv2.medianBlur(foreground_mask, 5) \n",
    "        # Reduce the noise \n",
    "\n",
    "        \n",
    "        _, foreground_mask = cv2.threshold(foreground_mask,127,255,cv2.THRESH_BINARY)\n",
    " \n",
    "        # Find the index of the largest contour and draw bounding box\n",
    "        foreground_max_border = foreground_mask\n",
    "\n",
    "        contours, hierarchy = cv2.findContours(foreground_max_border,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
    "\n",
    "        contours_area = [cv2.contourArea(cont_) for cont_ in contours]\n",
    "        \n",
    " \n",
    "        #<1 means no countours exsiting in the display_frame\n",
    "        if len(contours_area) < 1:\n",
    "            \n",
    "            #Showing resultant display_frame\n",
    "            cv2.imshow('display_frame',display_frame)\n",
    " \n",
    "           \n",
    "            \n",
    " \n",
    "        \n",
    "            continue\n",
    " \n",
    "        else:\n",
    "            #detecting moving object inside the display_frame.\n",
    "            \n",
    "\n",
    "            \n",
    "            if numpy.argmax(contours_area)>numpy.argmax(templte_image):\n",
    "\n",
    "                highest_val_index = numpy.argmax(templte_image)\n",
    "            else:\n",
    "                highest_val_index = numpy.argmax(contours_area)\n",
    "        #for the border around the mmoving object\n",
    "        # Detected the index of largest moving object (contour) and enclosing it inside the bounding boxes .\n",
    "            \n",
    "        _contours = contours[highest_val_index]\n",
    "        val_x,val_y,width,height  = cv2.boundingRect(_contours)\n",
    "        cv2.rectangle(display_frame,(val_x,val_y),(val_x+width,val_y+height ),(0,0,2000),1)\n",
    " \n",
    "        #for centre of the oject existing in the display_frame\n",
    "        # Retrieve the centre of moving object and coordinates of its centre\n",
    "        \n",
    "\n",
    "        beta_x = val_x + int(width/2)\n",
    "\n",
    "        beta_y = val_y + int(height /2)\n",
    "\n",
    "        cv2.circle(display_frame,(beta_x,beta_y),4,(255,255,255),-1)\n",
    " \n",
    "        #using put details to show the centre of object in side the display_frame\n",
    "        details = \"X: \"       + str(beta_x) + \", Y: \"           + str(beta_y)\n",
    "        cv2.putText(display_frame, details, (beta_x - 15, beta_y - 15),\n",
    "            cv2.FONT_HERSHEY_SCRIPT_COMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "        if return_vid:\n",
    "            vidio_output=cv2.resize(display_frame,(300,300))\n",
    "\n",
    "            output_car.write(vidio_output)\n",
    "\n",
    "        # showing frames \n",
    "        cv2.imshow('display_frame',display_frame)\n",
    " \n",
    "        #exiting the loop after pressing key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "            break\n",
    " \n",
    "    #releasing the output file , releasing the video file , destroy all windows at end of execution\n",
    "    video_file.release()\n",
    "    output_car.release()\n",
    "    cv2.destroyAllWindows()\n",
    " \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
